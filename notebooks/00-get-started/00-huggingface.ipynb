{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "This notebook demonstrates how to use transformers from [Hugging Face](https://huggingface.co/).\n",
    "\n",
    "### Setup\n",
    "[See the initial setup](../../README.md#setup) to start a virtual environment and install packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9995941519737244}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "response = classifier(\"Hugging Face allows me to automate tasks and unlocks new opportunities.\",)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation\n",
    "<smalL>Note: For open-end generation, HuggingFace will set the padding token ID to be equal to the end-of-sentence token ID, so this will configure that manually beforehand to avoid a warning message.</small>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'In the future, AI large language models will help to bring intelligent AI into the world. For example'}, {'generated_text': 'In the future, AI large language models will help to develop more of the capabilities of AI, including'}]\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "response = generator(\n",
    "    \"In the future, AI large language models will help to\",\n",
    "    max_length=20,\n",
    "    num_return_sequences=2,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id # set to avoid warning\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'This is sample code for zero-shot classification using Python.', 'labels': ['technology', 'business', 'weather', 'education', 'politics'], 'scores': [0.9024143815040588, 0.0479886494576931, 0.020019279792904854, 0.018473103642463684, 0.01110463123768568]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", \"facebook/bart-large-mnli\")\n",
    "\n",
    "response = classifier(\n",
    "    \"This is sample code for zero-shot classification using Python.\",\n",
    "    candidate_labels=[\"business\",\"education\",\"politics\",\"technology\",\"weather\"],\n",
    "    model=model\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9995941519737244}, {'label': 'POSITIVE', 'score': 0.9998742341995239}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "X_train = [\"Hugging Face allows me to automate tasks and unlocks new opportunities.\",\n",
    "           \"Hugging Face is pretty cool!\"]\n",
    "\n",
    "response = classifier(X_train)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 17662,  2227,  4473,  2033,  2000,  8285,  8585,  8518,  1998,\n",
      "         19829,  2015,  2047,  6695,  1012,   102],\n",
      "        [  101, 17662,  2227,  2003,  3492,  4658,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-3.7883,  4.0208],\n",
      "        [-4.2771,  4.7040]]), hidden_states=None, attentions=None)\n",
      "tensor([[4.0589e-04, 9.9959e-01],\n",
      "        [1.2575e-04, 9.9987e-01]])\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "print(batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    print(outputs)\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    print(predictions)\n",
    "    labels = torch.argmax(predictions, dim=1)\n",
    "    print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
